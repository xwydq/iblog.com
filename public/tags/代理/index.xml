<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>代理 on 飞舞的尘埃</title>
    <link>/tags/%E4%BB%A3%E7%90%86/</link>
    <description>Recent content in 代理 on 飞舞的尘埃</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 10 May 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/%E4%BB%A3%E7%90%86/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>R使用代理并行下载</title>
      <link>/post/2018/05/10/r-proxy-parallel-download/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/10/r-proxy-parallel-download/</guid>
      <description>在R中结合parallel包来实现并行下载或者请求多个URL地址以提高速度
 urls = c( &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220648.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220649.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220650.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220651.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220653.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220654.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220726.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220878.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220882.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220939.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220942.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220945.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220947.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204221018.PDF&amp;#34; ) library(parallel) library(httr) library(readr) # wget proxy str1 = &amp;#39;wget -e &amp;#34;http_proxy=%s&amp;#34; %s -O ~/test/annouce/%s.pdf --timeout=10 --tries=2&amp;#39; str1 = &amp;#39;wget %s -O ~/test/annouce/%s.pdf --timeout=10 --tries=2 --quiet&amp;#39; lturls = as.list(urls) t1 = Sys.time() mclapply(lturls, FUN = function(x){ print(x) ## update ips # if(runif(1) &amp;gt; 0.6){ # ips &amp;lt;&amp;lt;- updateIps(ips) # } fnm = digest(x) ## wget proxy # cmd_str = sprintf(str1, sample(ips, 1), x, fnm) cmd_str = sprintf(str1, x, fnm) system(cmd_str) ## rm file # system(sprintf(&amp;#39;rm ~/test/annouce/%s.</description>
    </item>
    
    <item>
      <title>R爬虫-代理使用</title>
      <link>/note/2018/04/12/r-crawl-http-proxy/</link>
      <pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/04/12/r-crawl-http-proxy/</guid>
      <description>R爬虫的利器rvest，可以方便实现数据的定位与提取
添加代理需在httr下实现
直接在请求中使用代理 library(httr) ## header 使用vector类型 h &amp;lt;- c(&amp;#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&amp;#34;, &amp;#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&amp;#34;, &amp;#34;gzip, deflate&amp;#34;, &amp;#34;keep-alive&amp;#34;) names(h) &amp;lt;- c(&amp;#34;User-Agent&amp;#34;, &amp;#34;Accept&amp;#34;, &amp;#34;Accept-Encoding&amp;#34;, &amp;#34;Connection&amp;#34;) ## 持续获取代理 get_ips = function(){ while(TRUE){ IP_proxy = &amp;#39;http://mvip.piping.mogumiao.com/proxy/api/get_ip_bs?appKey=41620212070f4853bf27fd12&amp;amp;count=20&amp;amp;expiryDate=0&amp;amp;format=1&amp;#39; ips = read_json(IP_proxy) if(ips$code == &amp;#39;0&amp;#39;){ break }else{ Sys.sleep(5) } } ips = ips[[&amp;#39;msg&amp;#39;]] ips = as.data.frame(do.call(rbind, ips)) ips$ip = as.character(ips$ip) ips$port = as.numeric(ips$port) return(ips) } ips = get_ips() # port ip # 1 39358 125.</description>
    </item>
    
    <item>
      <title>python爬虫-代理使用</title>
      <link>/note/2018/04/09/crawl-python-proxy/</link>
      <pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/04/09/crawl-python-proxy/</guid>
      <description>http代理是爬虫工作中解决反爬的一项关键措施，下面说明不同场景下代理的使用及其验证
 requests中使用代理  无加密代理  import requests proxy1 = {&amp;#39;http&amp;#39;: &amp;#39;117.90.51.49:42668&amp;#39;, &amp;#39;https&amp;#39;: &amp;#39;117.90.51.49:42668&amp;#39;} resp = requests.get(&amp;#39;http://httpbin.org/ip&amp;#39;, proxies=proxy1) print(resp.json())  需认证的代理  import requests proxy1 = {&amp;#39;http&amp;#39;: &amp;#39;http://user:passwd@106.15.95.226:9187&amp;#39;, &amp;#39;https&amp;#39;: &amp;#39;https://user:passwd@106.15.95.236:9187&amp;#39;} resp = requests.get(&amp;#39;http://httpbin.org/ip&amp;#39;, proxies=proxy1) print(resp.json()) # {&amp;#39;origin&amp;#39;: &amp;#39;101.47.19.29, 106.15.95.236&amp;#39;} selenium+浏览器中使用代理 phantomJS ## selenium+phantomJS代理 from selenium import webdriver driver = webdriver.PhantomJS( # executable_path = &amp;#39;/usr/local/bin/phantomjs&amp;#39;, service_args = [ &amp;#39;--ignore-ssl-errors=true&amp;#39;, &amp;#39;--proxy=106.15.95.236:9187&amp;#39;, # IP:port &amp;#39;--proxy-type=http&amp;#39;, &amp;#39;--proxy-auth=user:passwd&amp;#39;, # 如需认证添加 ]) url = &amp;#39;http://httpbin.org/ip&amp;#39; driver.</description>
    </item>
    
  </channel>
</rss>