<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on 飞舞的尘埃</title>
    <link>/categories/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on 飞舞的尘埃</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 10 May 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>R使用代理并行下载</title>
      <link>/post/2018/05/10/r-proxy-parallel-download/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/10/r-proxy-parallel-download/</guid>
      <description>在R中结合parallel包来实现并行下载或者请求多个URL地址以提高速度
 urls = c( &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220648.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220649.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220650.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220651.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220653.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220654.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220726.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220878.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220882.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220939.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220942.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220945.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204220947.PDF&amp;#34;, &amp;#34;http://www.cninfo.com.cn/finalpage/2017-12-14/1204221018.PDF&amp;#34; ) library(parallel) library(httr) library(readr) # wget proxy str1 = &amp;#39;wget -e &amp;#34;http_proxy=%s&amp;#34; %s -O ~/test/annouce/%s.pdf --timeout=10 --tries=2&amp;#39; str1 = &amp;#39;wget %s -O ~/test/annouce/%s.pdf --timeout=10 --tries=2 --quiet&amp;#39; lturls = as.list(urls) t1 = Sys.time() mclapply(lturls, FUN = function(x){ print(x) ## update ips # if(runif(1) &amp;gt; 0.6){ # ips &amp;lt;&amp;lt;- updateIps(ips) # } fnm = digest(x) ## wget proxy # cmd_str = sprintf(str1, sample(ips, 1), x, fnm) cmd_str = sprintf(str1, x, fnm) system(cmd_str) ## rm file # system(sprintf(&amp;#39;rm ~/test/annouce/%s.</description>
    </item>
    
    <item>
      <title>R爬虫-代理使用</title>
      <link>/note/2018/04/12/r-crawl-http-proxy/</link>
      <pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/04/12/r-crawl-http-proxy/</guid>
      <description>R爬虫的利器rvest，可以方便实现数据的定位与提取
添加代理需在httr下实现
直接在请求中使用代理 library(httr) ## header 使用vector类型 h &amp;lt;- c(&amp;#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&amp;#34;, &amp;#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&amp;#34;, &amp;#34;gzip, deflate&amp;#34;, &amp;#34;keep-alive&amp;#34;) names(h) &amp;lt;- c(&amp;#34;User-Agent&amp;#34;, &amp;#34;Accept&amp;#34;, &amp;#34;Accept-Encoding&amp;#34;, &amp;#34;Connection&amp;#34;) ## 持续获取代理 get_ips = function(){ while(TRUE){ IP_proxy = &amp;#39;http://mvip.piping.mogumiao.com/proxy/api/get_ip_bs?appKey=41620212070f4853bf27fd12&amp;amp;count=20&amp;amp;expiryDate=0&amp;amp;format=1&amp;#39; ips = read_json(IP_proxy) if(ips$code == &amp;#39;0&amp;#39;){ break }else{ Sys.sleep(5) } } ips = ips[[&amp;#39;msg&amp;#39;]] ips = as.data.frame(do.call(rbind, ips)) ips$ip = as.character(ips$ip) ips$port = as.numeric(ips$port) return(ips) } ips = get_ips() # port ip # 1 39358 125.</description>
    </item>
    
    <item>
      <title>python爬虫-代理使用</title>
      <link>/note/2018/04/09/crawl-python-proxy/</link>
      <pubDate>Mon, 09 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/04/09/crawl-python-proxy/</guid>
      <description>http代理是爬虫工作中解决反爬的一项关键措施，下面说明不同场景下代理的使用及其验证
 requests中使用代理  无加密代理  import requests proxy1 = {&amp;#39;http&amp;#39;: &amp;#39;117.90.51.49:42668&amp;#39;, &amp;#39;https&amp;#39;: &amp;#39;117.90.51.49:42668&amp;#39;} resp = requests.get(&amp;#39;http://httpbin.org/ip&amp;#39;, proxies=proxy1) print(resp.json())  需认证的代理  import requests proxy1 = {&amp;#39;http&amp;#39;: &amp;#39;http://user:passwd@106.15.95.226:9187&amp;#39;, &amp;#39;https&amp;#39;: &amp;#39;https://user:passwd@106.15.95.236:9187&amp;#39;} resp = requests.get(&amp;#39;http://httpbin.org/ip&amp;#39;, proxies=proxy1) print(resp.json()) # {&amp;#39;origin&amp;#39;: &amp;#39;101.47.19.29, 106.15.95.236&amp;#39;} selenium+浏览器中使用代理 phantomJS ## selenium+phantomJS代理 from selenium import webdriver driver = webdriver.PhantomJS( # executable_path = &amp;#39;/usr/local/bin/phantomjs&amp;#39;, service_args = [ &amp;#39;--ignore-ssl-errors=true&amp;#39;, &amp;#39;--proxy=106.15.95.236:9187&amp;#39;, # IP:port &amp;#39;--proxy-type=http&amp;#39;, &amp;#39;--proxy-auth=user:passwd&amp;#39;, # 如需认证添加 ]) url = &amp;#39;http://httpbin.org/ip&amp;#39; driver.</description>
    </item>
    
    <item>
      <title>爬虫：AMAC私募基金管理人</title>
      <link>/note/2018/03/11/crawl-amac-fund-manager/</link>
      <pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/03/11/crawl-amac-fund-manager/</guid>
      <description>基金业协会备案信息之：基金管理人 模块引入 import requests import pandas as pd import random import math import time import datetime from sqlalchemy import create_engine from lxml import etree import re requests.__version__ # &amp;#39;2.18.4&amp;#39; 基金管理人列表 chrome打开network查看请求，具体请求API为 POST，接口：http://gs.amac.org.cn/amac-infodisc/api/pof/manager，参数：?rand=0.24323480064904235&amp;amp;page=1&amp;amp;size=20
 rand：0-1随机数 page：页码(第1页：page=0) size：每页显示的记录数  参数说明：
 Query String Parameters: params，查询参数 Request Payload: json，为空{}时，也需要添加该参数，否则有可能报错  # url = &amp;#39;http://gs.amac.org.cn/amac-infodisc/api/pof/manager?rand=0.24121315630506435&amp;amp;page=0&amp;amp;size=20&amp;#39; url = &amp;#39;http://gs.amac.org.cn/amac-infodisc/api/pof/manager&amp;#39; paras = {&amp;#34;rand&amp;#34;: 0.24121315630506435, &amp;#34;page&amp;#34;: 4, &amp;#34;size&amp;#34;: 2} headers = { &amp;#39;Content-Type&amp;#39;: &amp;#39;application/json&amp;#39; } rs = requests.</description>
    </item>
    
    <item>
      <title>python爬虫:网页重定向问题</title>
      <link>/note/2018/03/05/crawler-url-redirect/</link>
      <pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/03/05/crawler-url-redirect/</guid>
      <description>使用python+requests爬虫时常常遇到请求URL地址变化（返回的URL地址不再是请求时的地址），这些很大可能是网页被重定向导致。所谓重定向(Redirect)就是通过各种方法将各种网络请求重新转到其它位置（URL）。每个网站主页是网站资源的入口，当重定向发生在网站主页时，如果不能正确处理就很有可能会错失这整个网站的内容。
 爬取网页时主要三种重定向的情况
  服务器端重定向，在服务器端完成，一般来说爬虫可以自适应，是不需要特别处理的，如响应代码301（永久重定向）、302（暂时重定向）等。具体来说，可以通过requests请求得到的response对象中的url、status_code两个属性来判断。当status_code为301、302或其他代表重定向的代码时，表示原请求被重定向；当response对象的url属性与发送请求时的链接不一致时，也说明了原请求被重定向且已经自动处理。
  客户端重定向：请求头meta refresh设置，即网页中的&amp;lt;meta&amp;gt;标签声明了网页重定向的链接，这种重定向由浏览器完成，需要编写代码进行处理。例如，某一重定向如下面的html代码第三行中的注释所示，浏览器能够自动跳转，但爬虫只能得到跳转前的页面，不能自动跳转。
如百度搜索requests后第一个结果地址https://www.baidu.com/link?url=n2d6IqviMKE2UKdm3cJo02edoksu6FX81jzThBQbkehNlFLpXO18Wry6_S3p_sp8&amp;amp;wd=&amp;amp;eqid=9b51b77c000016fb000000045a9ca929这个地址会跳转到http://www.python-requests.org/
  &amp;lt;meta http-equiv=&amp;#34;refresh&amp;#34; content=&amp;#34;0.1;url=http://www.redirectedtoxxx.com/&amp;#34;&amp;gt;&amp;lt;!--本网页会在0.1秒内refresh为url所指的网页--&amp;gt; &amp;lt;meta content=&amp;#34;always&amp;#34; name=&amp;#34;referrer&amp;#34;&amp;gt; &amp;lt;script&amp;gt;try{if(window.opener&amp;amp;&amp;amp;window.opener.bds&amp;amp;&amp;amp;window.opener.bds.pdc&amp;amp;&amp;amp;window.opener.bds.pdc.sendLinkLog){window.opener.bds.pdc.sendLinkLog();}}catch(e) {};var timeout = 0;if(/bdlksmp/.test(window.location.href)){var reg = /bdlksmp=([^=&amp;amp;]+)/,matches = window.location.href.match(reg);timeout = matches[1] ? matches[1] : 0};setTimeout(function(){window.location.replace(&amp;#34;http://www.python-requests.org/&amp;#34;)},timeout);window.opener=null;&amp;lt;/script&amp;gt; &amp;lt;noscript&amp;gt; &amp;lt;META http-equiv=&amp;#34;refresh&amp;#34; content=&amp;#34;0;URL=&amp;#39;http://www.python-requests.org/&amp;#39;&amp;#34;&amp;gt; &amp;lt;/noscript&amp;gt; 这种行为发生在客户端（浏览器），所以使用python requests 时不能实现自动跳转，返回结果仍然是原始URL地址。
import requests url = &amp;#39;https://www.baidu.com/link?url=n2d6IqviMKE2UKdm3cJo02edoksu6FX81jzThBQbkehNlFLpXO18Wry6_S3p_sp8&amp;amp;wd=&amp;amp;eqid=9b51b77c000016fb000000045a9ca929&amp;#39; r = requests.get(url) r.status_code #200 r.url #&amp;#39;https://www.baidu.com/link?url=n2d6IqviMKE2UKdm3cJo02edoksu6FX81jzThBQbkehNlFLpXO18Wry6_S3p_sp8&amp;amp;wd=&amp;amp;eqid=9b51b77c000016fb000000045a9ca929&amp;#39; 解决方法：获取真正要请求的URL，再重新requests
# xpath(&amp;#39;//meta[@http-equiv=&amp;#34;refresh&amp;#34; and @content]/@content&amp;#39;)提取出content的值 # 正则表达式提取出重定向的url import requests import re from lxml import etree def find_RealURL(url): r = requests.</description>
    </item>
    
    <item>
      <title>centos7安装squid实现http代理服务</title>
      <link>/note/2018/03/01/centos7-squid-http/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/03/01/centos7-squid-http/</guid>
      <description>在centos7环境搭建带认证功能的squid代理服务
 yum安装 squid实现代理功能，httpd-tools文件加密
yum -y install squid yum -y install httpd-tools squid的使用参数 进程启动、状态控制
systemctl enable squid systemctl status/restart/stop squid systemctl status squid ● squid.service - Squid caching proxy Loaded: loaded (/usr/lib/systemd/system/squid.service; enabled; vendor preset: disabled) Active: active (running) since 三 2018-02-28 05:56:10 CST; 23h ago Process: 4889 ExecStop=/usr/sbin/squid -k shutdown -f $SQUID_CONF (code=exited, status=0/SUCCESS) Process: 4898 ExecStart=/usr/sbin/squid $SQUID_OPTS -f $SQUID_CONF (code=exited, status=0/SUCCESS) Process: 4892 ExecStartPre=/usr/libexec/squid/cache_swap.sh (code=exited, status=0/SUCCESS) Main PID: 4900 (squid) CGroup: /system.</description>
    </item>
    
    <item>
      <title>linux环境下selenium操作Chrome</title>
      <link>/note/2018/02/27/linux-selenium-chromeheadless-install-test/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/2018/02/27/linux-selenium-chromeheadless-install-test/</guid>
      <description>typora-copy-images-to: ipic
  centos7及以上Linux环境。使用PhantomJS设置cookie报错Can only set Cookies for the current domain，看样子后期相关维护也很慢。google chrome已经开始支持headless，可以考虑换大腿了
 环境配置安装 可以直接使用配置yum源的方法来安装，但是由于伟大的墙可能访问不到Google服务，也可以先下载指定版本的安装文件再安装
  chromedriver安装
$ wget https://chromedriver.storage.googleapis.com/2.35/chromedriver_linux64.zip $ unzip chromedriver_linux64.zip $ chromedriver --version #ChromeDriver 2.35.528139 # 将chromedriver执行文件所在的路径添加到PATH环境变量   chromebrowser安装
$ wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm $ sudo yum install google-chrome-stable_current_x86_64.rpm $ google-chrome --version #Google Chrome 61.0.3163.100   Xvfb安装
Xvfb（virtual framebuffer X server for X Version 11）， Xvfb 可以直接处理 Window 的图形化功能，且不把图像輸出到屏幕，目的就是在没有 Xwindow的情况下仍然可以执行相关图像界面操作。实现一种虚拟窗口（window）的操作。安裝 XVFB 做 Selenium 測試</description>
    </item>
    
  </channel>
</rss>